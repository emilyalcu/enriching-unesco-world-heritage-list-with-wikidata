{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART ONE: CREATE UNESCO JSON FILE (unesco_not_enriched.json)\n",
    "\n",
    "- Convert XML file from UNESCO site (https://whc.unesco.org/en/syndication) into a list of dictionaries in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1, Step 1: loop through each element in each row/site and print all elements tags\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "tree = etree.parse('whc_en.xml')\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "for row in root:\n",
    "    for element in row:\n",
    "        print(element.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1, Step 2: for each row(site), store the elements and attributes in a dictionary. Print and dump into a JSON file (unesco_not_enriched.json)\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "import json\n",
    "\n",
    "tree = etree.parse('whc_en.xml')\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "# create list to store all wh_site dictionaries\n",
    "wh_sites = []\n",
    "\n",
    "for row in root.findall('row'):\n",
    "    wh_site_info={}\n",
    "    wh_site_info['id_number'] = row.find('id_number').text\n",
    "    wh_site_info['site'] = row.find('site').text\n",
    "    wh_site_info['category'] = row.find('category').text\n",
    "    wh_site_info['criteria_txt'] = row.find('criteria_txt').text\n",
    "    wh_site_info['danger'] = row.find('danger').text\n",
    "    wh_site_info['date_inscribed'] = row.find('date_inscribed').text\n",
    "    wh_site_info['extension'] = row.find('extension').text\n",
    "    wh_site_info['http_url'] = row.find('http_url').text\n",
    "    wh_site_info['image_url'] = row.find('image_url').text\n",
    "    wh_site_info['iso_code'] = row.find('iso_code').text\n",
    "    wh_site_info['justification'] = row.find('justification').text\n",
    "    wh_site_info['latitude'] = row.find('latitude').text\n",
    "    wh_site_info['longitude'] = row.find('longitude').text\n",
    "    wh_site_info['location'] = row.find('location').text\n",
    "    wh_site_info['region'] = row.find('region').text\n",
    "    wh_site_info['revision'] = row.find('revision').text\n",
    "    wh_site_info['secondary_dates'] = row.find('secondary_dates').text\n",
    "    wh_site_info['short_description'] = row.find('short_description').text\n",
    "    wh_site_info['states'] = row.find('states').text\n",
    "    wh_site_info['transboundary'] = row.find('transboundary').text\n",
    "    wh_site_info['unique_number'] = row.find('unique_number').text\n",
    "    \n",
    "\n",
    "    print(wh_site_info)\n",
    "\n",
    "    wh_sites.append(wh_site_info)\n",
    "\n",
    "with open('unesco_not_enriched.json', 'w') as json_file:\n",
    "    json.dump(wh_sites, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: QUERY WIKIDATA AND STORE RESULTS IN A JSON FILE (wikidata_query_not_formatted.json)\n",
    "\n",
    "- write SPARQL query using wikidata query service\n",
    "- nest SPARQL query in script\n",
    "- store results in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2, Step 1: Print the results from the SPARQL query as JSON dictionaries and save to a JSON file (wikidata_wrong_format.json)\n",
    "# query is copied over from https://query.wikidata.org/#%23CONCATINATED%20-%20INSTANCE%2C%20CULTURE%2C%20AREA%2C%20UNIT%2C%20AREA%20IN%20SQ%20METERS%20%28%0A%0ASELECT%20%3Fitem%20%3FitemLabel%20%3FWHID%20%28GROUP_CONCAT%28DISTINCT%20%3FinstanceOfLabel%3B%20SEPARATOR%3D%22%2C%20%22%29%20AS%20%3FWHS_instanceOf%29%20%0A%28GROUP_CONCAT%28DISTINCT%20%3FcultureLabel%3B%20SEPARATOR%3D%22%2C%20%22%29%20AS%20%3FWHS_culture%29%0A%28GROUP_CONCAT%28DISTINCT%20%3FareaInSqMeters%3B%20SEPARATOR%3D%22%2C%20%22%29%20AS%20%3FWHS_area_sq_meters%29%0AWHERE%0A%7B%0A%20%20%3Fitem%20wdt%3AP1435%20wd%3AQ9259.%20%23%20has%20heritage%20designation%20of%20World%20Heritage%20Site%0A%20%20%3Fitem%20wdt%3AP757%20%3FWHID.%0A%20%20optional%7B%0A%20%20%20%20%3Fitem%20wdt%3AP31%20%3FinstanceOf.%0A%20%20%20%20%7D%0A%20%20optional%7B%0A%20%20%3Fitem%20wdt%3AP2596%20%3Fculture%0A%20%20%20%20%7D%0A%20%20optional%7B%0A%20%20%3Fitem%20p%3AP2046%20%3Fstmnode.%20%23area%0A%20%20%3Fstmnode%20psv%3AP2046%20%3Fvaluenode.%0A%20%20%3Fvaluenode%20wikibase%3AquantityAmount%20%3Farea.%0A%20%20%3Fvaluenode%20wikibase%3AquantityUnit%20%3Funit.%0A%20%20%3Funit%20p%3AP2370%20%3Funitstmnode.%0A%20%20%3Funitstmnode%20psv%3AP2370%20%3Funitvaluenode.%20%0A%20%20%3Funitvaluenode%20wikibase%3AquantityAmount%20%3Fconversion.%0A%20%20%3Funitvaluenode%20wikibase%3AquantityUnit%20wd%3AQ25343.%0A%20%20BIND%28%3Farea%20%2a%20%3Fconversion%20AS%20%3FareaInSqMeters%29.%0A%20%20%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%0A%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22%5BAUTO_LANGUAGE%5D%2Cen%22.%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3FinstanceOf%20rdfs%3Alabel%20%3FinstanceOfLabel.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Fculture%20rdfs%3Alabel%20%3FcultureLabel.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Fitem%20rdfs%3Alabel%20%3FitemLabel.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7D%20%0A%7D%20%0A%0AGROUP%20BY%20%3Fitem%20%3FitemLabel%20%3FWHID%0AORDER%20BY%20%3FWHID%0A\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# SPARQL QUERY VARIABLE\n",
    "sparql = '''\n",
    "SELECT ?id_number (?item AS ?wikidata_URI) (?itemLabel AS ?wikidata_label) \n",
    "(GROUP_CONCAT(DISTINCT ?instanceOf; SEPARATOR=\", \") AS ?wikidata_instance_of_URI) \n",
    "(GROUP_CONCAT(DISTINCT ?instanceOfLabel; SEPARATOR=\", \") AS ?wikidata_instance_of_label) \n",
    "(GROUP_CONCAT(DISTINCT ?culture; SEPARATOR=\", \") AS ?wikidata_culture_URI)\n",
    "(GROUP_CONCAT(DISTINCT ?cultureLabel; SEPARATOR=\", \") AS ?wikidata_culture_label)\n",
    "(GROUP_CONCAT(DISTINCT ?areaInSqMeters; SEPARATOR=\", \") AS ?wikidata_area_in_sq_meters)\n",
    "WHERE\n",
    "{\n",
    "  ?item wdt:P1435 wd:Q9259. # has heritage designation of World Heritage Site\n",
    "  ?item wdt:P757 ?id_number.\n",
    "  optional{\n",
    "    ?item wdt:P31 ?instanceOf.\n",
    "    }\n",
    "  optional{\n",
    "  ?item wdt:P2596 ?culture\n",
    "    }\n",
    "   optional {\n",
    "        ?item p:P2046 ?statement.\n",
    "        ?statement psv:P2046 ?valuenode.\n",
    "        ?valuenode wikibase:quantityUnit ?unit.\n",
    "        ?valuenode wikibase:quantityAmount ?area.\n",
    "        ?statement pq:P518 wd:Q9259.\n",
    "        ?unit p:P2370 ?unitstmnode.\n",
    "        ?unitstmnode psv:P2370 ?unitvaluenode. \n",
    "        ?unitvaluenode wikibase:quantityAmount ?conversion.\n",
    "        ?unitvaluenode wikibase:quantityUnit wd:Q25343.\n",
    "        BIND(?area * ?conversion AS ?areaInSqMeters).\n",
    "  }\n",
    "                         \n",
    " SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". \n",
    "                        ?instanceOf rdfs:label ?instanceOfLabel.\n",
    "                        ?culture rdfs:label ?cultureLabel.\n",
    "                        ?item rdfs:label ?itemLabel.\n",
    "                        } \n",
    "} \n",
    "\n",
    "GROUP BY ?id_number ?item ?itemLabel \n",
    "ORDER BY ?id_number\n",
    "'''\n",
    "\n",
    "params = {\n",
    "    'query' : sparql\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Accept' : 'application/json',\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "r = requests.get(url, params = params, headers=headers)\n",
    "\n",
    "wikidata = json.loads(r.text)\n",
    "\n",
    "query_results = []\n",
    "\n",
    "for result in wikidata ['results']['bindings']:\n",
    "    query_results.append(result)\n",
    "\n",
    "query_results_json = json.dumps(query_results, indent =2)\n",
    "\n",
    "print(query_results_json)\n",
    "\n",
    "with open(\"wikidata_query_not_formatted.json\", 'w') as json_file:\n",
    "    json.dump(query_results, json_file, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2, step 2: Reformat the JSON file to match the format of the unesco_not_enriched.json file. Print and save as a JSON file (wikidata.json)\n",
    "\n",
    "import json\n",
    "with open('wikidata_query_not_formatted.json', 'r') as wikidatafile:\n",
    "    wikidata = json.load(wikidatafile)\n",
    "\n",
    "consolidated_data = []\n",
    "\n",
    "for wikidata_site in wikidata:\n",
    "    consolidated_item = {}\n",
    "\n",
    "    for key, value in wikidata_site.items():\n",
    "        if key != \"type\":\n",
    "            consolidated_item[key] = value[\"value\"]\n",
    "\n",
    "    consolidated_data.append(consolidated_item)\n",
    "\n",
    "for result in consolidated_data:\n",
    "    print(result)\n",
    "\n",
    "with open('wikidata.json', 'w') as wikidata_reformatted_json:\n",
    "    json.dump(consolidated_data, wikidata_reformatted_json, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3: MERGE THE TWO JSON FILES \n",
    "\n",
    "- Merge unesco_not_formatted.json and wikidata.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3, Step 1: Merge unesco_not_formatted.json and wikidata.json on the id_number\n",
    "\n",
    "import json\n",
    "\n",
    "def merge_json_files(file1_path, file2_path):\n",
    "    merged_data = {}\n",
    "\n",
    "    # Load data from unesco_not_enriched.json file\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        data1 = json.load(file1)\n",
    "        for item in data1:\n",
    "            item_id = item['id_number']\n",
    "            if item_id not in merged_data:\n",
    "                merged_data[item_id] = item\n",
    "            else:\n",
    "                # Merge fields from unesco_not_enriched.json into merged_data\n",
    "                for key, value in item.items():\n",
    "                    if key not in merged_data[item_id]:\n",
    "                        merged_data[item_id][key] = value\n",
    "\n",
    "    # Load data from wikidata.json file and fill missing data\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        data2 = json.load(file2)\n",
    "        for item in data2:\n",
    "            item_id = item['id_number']\n",
    "            if item_id not in merged_data:\n",
    "                # Fill missing fields with default values - make a note that the there is not a maching id_number in the unesco dataset\n",
    "                default_values = {\"issue\": \"id_number not in unesco data\"}  \n",
    "                merged_data[item_id] = {**default_values, **item}\n",
    "            else:\n",
    "                # Merge fields from wikidata.json into merged_data\n",
    "                for key, value in item.items():\n",
    "                    if key not in merged_data[item_id]:\n",
    "                        merged_data[item_id][key] = value\n",
    "\n",
    "    return list(merged_data.values())  # Convert dictionary values to list\n",
    "\n",
    "file1_path = \"unesco_not_enriched.json\"\n",
    "file2_path = \"wikidata.json\"\n",
    "output_file = \"unesco_enriched_not_cleaned.json\"\n",
    "\n",
    "# Merge the 2 JSON files\n",
    "merged_data = merge_json_files(file1_path, file2_path)\n",
    "\n",
    "# Write the merged data to a new JSON file\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(merged_data, outfile, indent=4)\n",
    "\n",
    "print(\"Merged data saved successfully to\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 4: CLEAN THE ENRICHED DATASET\n",
    "\n",
    "- Merge and delete records using OpenRefine\n",
    "- Export the file from OpenRefine as a CSV\n",
    "- Convert the CSV to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4, Step 1: Merge and delete rows in OpenRefine\n",
    "\n",
    "1. open unesco_enriched_not_cleaned.json in OpenRefine; \n",
    "    - check preserve empty strings\n",
    "    - check trim leading and trailing whitespace from strings\n",
    "    - create project\n",
    "\n",
    "2. sort id_number row a-z\n",
    "\n",
    "3. split \"id_number\" column. some id_numbers have a suffix (quarter, ter, bis, etc) when a single site has been given more than one id_number. this step removes the suffix so we can group duplicate records.\n",
    "    - edit column > split into several columns > separator = [a-zA-Z]\n",
    "    - check regular expression\n",
    "    - split into 2 columns at most\n",
    "    - uncheck \"remove this column\"\n",
    "\n",
    "4. Change sort\n",
    "    - remove sort on \"id number\"\n",
    "    - \"id_number 1\" >  sort by number; smallest first\n",
    "    - make sort permanent \n",
    "\n",
    "5. remove whitespace \n",
    "    - \"id_number 1\" > remove trailing / leading whitespace \n",
    "\n",
    "6. get list of all records with duplicate id_numbers\n",
    "    - \"id_number 1\" > text facet \n",
    "    -  in facet window:\n",
    "        - sort by count \n",
    "        - click \"include\" on \"id_numbers 1\" that have over 1 instance\n",
    "    - this gives you a list of records with duplicate id_numbers\n",
    "    - the unesco record is above the wikidata record \n",
    "\n",
    "7. the next part requires a discerning eye. \n",
    "    - When there are two instances of one id_number: \n",
    "        - If the UNESCO record (top) has values in the wikidata fields, the corresponding wikidata record (bottom) can be deleted. Flag the wikidata record to be deleted later. \n",
    "        - If the UNESCO record (top) does not have values in the wikidata fields, the corresponding wikidata record can be merged with the UNESCO record. Star the UNESCO record and the wikidata record to merge later. \n",
    "    - when there are more than two instances:\n",
    "        - Situtation 1: The UNESCO site may have have been given multiple id_numbers over the years. The change in id_numbers over time does not affect the wikidata \"instance_of,\" \"culture,\" or \"area\" values. Wikidata records with repeated id_numbers and duplicated wikidata values can be flagged and deleted for later. \n",
    "        - Stiatuion 2: Some larger world heritage sites have consist of many different structures or parts (buildings, churches, memorials, etc). Sometimes these \"sub-sections\" are given their own record in wikidata. These sub-sections are not within the scope of this project. id_numbers of subsections can have suffixes (quarter, bis, ter, etc) or they can have dashes (main site: 404; sub-sections: 404-001, 404-002, 404-003). Flag sub-sections and delete later. \n",
    "        \n",
    "8. delete flagged items\n",
    "    - remove all facets \n",
    "    - all > facet > facet by flag \n",
    "    - in facet window: if true include\n",
    "    - all > edit rows > remove matching rows\n",
    "\n",
    "7. merge starred items \n",
    "    - remove all facets \n",
    "    - all > facet > facet by star\n",
    "    - in facet window: if true include\n",
    "    - id_number 1 is already permanetely sorted a-z\n",
    "    - go to \"show as rows\"\n",
    "    - \"id_number 1\" > edit cells > blank down\n",
    "    - go to \"show as records\" (records with the same \"id_number 1\" will be grouped together)\n",
    "    - for each column: edit cells > join multi-valued cells\n",
    "    - the unesco and wikidata records with the same \"id_number_1\" will be merged\n",
    "\n",
    "8. search for more repeated records in the wikidata_label column\n",
    "    - wikidata_label > text facet \n",
    "    - in facet window: sort by count \n",
    "    - wikidata_label > sort a-z\n",
    "    - flag and delete repeated records (same method as above)\n",
    "    - star and merge unesco and wikidata records that should be merged (same method as above)\n",
    "\n",
    "9. search for more UNESCO sites that have sub-locations wikidata records\n",
    "    - id_number > facet by text\n",
    "    - In facet window browse for id_numbers that are followed by a dash and more numbers (404-001, 404-002, etc)\n",
    "    - click \"include\" to get a list of sub-section records \n",
    "    - flag and delete\n",
    "\n",
    "10. remove HTML tags \n",
    "    - short_description > transform > GREL: value.replace(/<[^>]*>/,\"\")\n",
    "    - justification > transform > GREL: value.replace(/<[^>]*>/,\"\")\n",
    "\n",
    "11. remove \"issue\" column\n",
    "\n",
    "12. make area values numbers\n",
    "    - wikidata_area_in_sq_meters > edit cells > common transformations > to number \n",
    "\n",
    "13. rename and reorder columns if necessary\n",
    "\n",
    "14. export to CSV (unesco_enriched_cleaned_all.csv)\n",
    "\n",
    "15. search for records without coordinates\n",
    "    - latitude > facet by text \n",
    "    - in facet window: find (blank) instances and click include\n",
    "    - flag and delete\n",
    "\n",
    "16. export to CSV (unesco_enriched_cleaned_with_coordinates_only.csv)\n",
    "\n",
    "17. search for records without area value\n",
    "    - wikidata_area_in_sq_meters > facet by number \n",
    "    - in facet window: find (blank) instances and click include\n",
    "    - flag and delete\n",
    "\n",
    "18. export to CSV (unesco_enriched_cleaned_with_area_only.csv)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 5: CONVERT OPENREFINE CSVS INTO JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5, Step 1: convert unesco_enriched_cleaned_all.csv into a JSON file\n",
    "    # my csvs with transformed data from OpenRefine are in the github repository\n",
    "\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def make_json(csvFilePath, jsonFilePath):\n",
    "\n",
    "    cleaned_up_json_data = []\n",
    "\n",
    "    with open(csvFilePath, 'r') as cleaned_up_csv_file:\n",
    "        csvReader = csv.DictReader(cleaned_up_csv_file)\n",
    "\n",
    "        for row in csvReader:\n",
    "            cleaned_up_json_data.append(row)\n",
    "\n",
    "\n",
    "    with open(jsonFilePath, 'w') as cleaned_up_json_file:\n",
    "        cleaned_up_json_file.write(json.dumps(cleaned_up_json_data, indent=2))\n",
    "\n",
    "csvFilePath = r\"unesco_enriched_cleaned_all.csv\"\n",
    "jsonFilePath = r\"FINAL_unesco_enriched_all.json\"\n",
    "\n",
    "make_json(csvFilePath, jsonFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5, Step 2: convert unesco_enriched_cleaned_coordinates.csv into a JSON file (coordinates.json)\n",
    "    # Only one site does not have coordinates, Funerary and memory sites of the First World War (Western Front)) --  it includes a series of 139 funerary and memorial sites\n",
    "    # This JSON file will be used to create a map visualization \n",
    "    # my csvs with transformed data from OpenRefine are in the github repository\n",
    "\n",
    "\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def make_json(csvFilePath, jsonFilePath):\n",
    "\n",
    "    cleaned_up_json_data = []\n",
    "\n",
    "    with open(csvFilePath, 'r') as cleaned_up_csv_file:\n",
    "        csvReader = csv.DictReader(cleaned_up_csv_file)\n",
    "\n",
    "        for row in csvReader:\n",
    "            cleaned_up_json_data.append(row)\n",
    "\n",
    "\n",
    "    with open(jsonFilePath, 'w') as cleaned_up_json_file:\n",
    "        cleaned_up_json_file.write(json.dumps(cleaned_up_json_data, indent=2))\n",
    "\n",
    "csvFilePath = r\"unesco_enriched_cleaned_coordinates.csv\"\n",
    "jsonFilePath = r\"coordinates.json\"\n",
    "\n",
    "make_json(csvFilePath, jsonFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5, Step 3: convert unesco_enriched_cleaned_area.csv into a JSON file (area.json)\n",
    "    # 628 sites have an area value, 569 site do not\n",
    "    # this JSON file will be used to create a visulization which will show the relative area of each site that has an area value\n",
    "\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def make_json(csvFilePath, jsonFilePath):\n",
    "\n",
    "    cleaned_up_json_data = []\n",
    "\n",
    "    with open(csvFilePath, 'r') as cleaned_up_csv_file:\n",
    "        csvReader = csv.DictReader(cleaned_up_csv_file)\n",
    "\n",
    "        for row in csvReader:\n",
    "            cleaned_up_json_data.append(row)\n",
    "\n",
    "\n",
    "    with open(jsonFilePath, 'w') as cleaned_up_json_file:\n",
    "        cleaned_up_json_file.write(json.dumps(cleaned_up_json_data, indent=2))\n",
    "\n",
    "csvFilePath = r\"unesco_enriched_cleaned_area.csv\"\n",
    "jsonFilePath = r\"area.json\"\n",
    "\n",
    "make_json(csvFilePath, jsonFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 6: CREATE VISUALIZATIONS\n",
    "- Convert JSON files into GEOJSON\n",
    "- coordinates.geojson & area.geojson\n",
    "- Upload datasets to MapBox Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6, Step 1: Convert coordinates dataset from JSON to GEOJSON\n",
    "    # coordinates.json > coordinates.geojson\n",
    "import json\n",
    "import geojson\n",
    "\n",
    "features = []\n",
    "\n",
    "with open('coordinates.json', 'r') as final_json_file:\n",
    "    final_data = json.load(final_json_file)\n",
    "\n",
    "print(final_data)\n",
    "\n",
    "for item in final_data:\n",
    "   longitude = float(item.get(\"longitude\", 0)) if item.get(\"longitude\") is not None else None\n",
    "   latitude = float(item.get(\"latitude\", 0)) if item.get(\"latitude\") is not None else None\n",
    "   feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [longitude, latitude]  # GeoJSON expects coordinates in [longitude, latitude] format\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"id_number\": item.get(\"id_number\", \"\"),\n",
    "            \"site\": item.get(\"site\", \"\"),\n",
    "            \"category\": item.get(\"category\", \"\"),\n",
    "            \"short_description\": item.get(\"short_description\", \"\"),\n",
    "            \"criteria_txt\": item.get(\"criteria_txt\", \"\"),\n",
    "            \"justification\": item.get(\"justification\", \"\"),\n",
    "            \"danger\": item.get(\"danger\", \"\"),\n",
    "            \"region\": item.get(\"region\", \"\"),\n",
    "            \"iso_code\": item.get(\"iso_code\", \"\"),\n",
    "            \"states\": item.get(\"states\", \"\"),\n",
    "            \"location\": item.get(\"location\", \"\"),\n",
    "            \"transboundary\": item.get(\"transboundary\", \"\"),\n",
    "            \"date_inscribed\": item.get(\"date_inscribed\", \"\"),\n",
    "            \"secondary_dates\": item.get(\"secondary_dates\", \"\"),\n",
    "            \"revision\": item.get(\"revision\", \"\"),\n",
    "            \"extension\": item.get(\"extension\", \"\"),\n",
    "            \"unique_number\": item.get(\"unique_number\", \"\"),\n",
    "            \"http_url\": item.get(\"http_url\", \"\"),\n",
    "            \"image_url\": item.get(\"image_url\", \"\"),\n",
    "            \"wikidata_URI\": item.get(\"wikidata_URI\", \"\"),  # If wikidata_URI is missing, it will default to an empty string\n",
    "            \"wikidata_label\": item.get(\"wikidata_Label\", \"\"),\n",
    "            \"wikidata_instance_of_URI\": item.get(\"wikidata_instance_of_URI\", \"\"),\n",
    "            \"wikidata_instance_of_label\": item.get(\"wikidata_instance_of\", \"\"),\n",
    "            \"wikidata_culture_URI\": item.get(\"wikidata_culture_URI\", \"\"),\n",
    "            \"wikidata_culture_label\": item.get(\"wikidata_culture\", \"\"),\n",
    "            \"wikidata_area_in_sq_meters\": item.get(\"wikidata_area_in_sq_meters\", \"\")\n",
    "        }\n",
    "    }\n",
    "   features.append(feature)\n",
    "\n",
    "# Create a FeatureCollection\n",
    "feature_collection = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": features\n",
    "}\n",
    "\n",
    "# Save GeoJSON to a file\n",
    "with open('coordinates.geojson', 'w') as f:\n",
    "    geojson.dump(feature_collection, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6, Step 2: Convert area dataset from JSON to GEOJSON\n",
    "    # area.json > area.geojson\n",
    "\n",
    "\n",
    "import json\n",
    "import geojson\n",
    "\n",
    "features = []\n",
    "\n",
    "with open('area.json', 'r') as final_json_file:\n",
    "    final_data = json.load(final_json_file)\n",
    "\n",
    "print(final_data)\n",
    "\n",
    "for item in final_data:\n",
    "   longitude = float(item.get(\"longitude\", 0)) if item.get(\"longitude\") is not None else None\n",
    "   latitude = float(item.get(\"latitude\", 0)) if item.get(\"latitude\") is not None else None\n",
    "   feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [longitude, latitude]  # GeoJSON expects coordinates in [longitude, latitude] format\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"id_number\": item.get(\"id_number\", \"\"),\n",
    "            \"site\": item.get(\"site\", \"\"),\n",
    "            \"category\": item.get(\"category\", \"\"),\n",
    "            \"short_description\": item.get(\"short_description\", \"\"),\n",
    "            \"criteria_txt\": item.get(\"criteria_txt\", \"\"),\n",
    "            \"justification\": item.get(\"justification\", \"\"),\n",
    "            \"danger\": item.get(\"danger\", \"\"),\n",
    "            \"region\": item.get(\"region\", \"\"),\n",
    "            \"iso_code\": item.get(\"iso_code\", \"\"),\n",
    "            \"states\": item.get(\"states\", \"\"),\n",
    "            \"location\": item.get(\"location\", \"\"),\n",
    "            \"transboundary\": item.get(\"transboundary\", \"\"),\n",
    "            \"date_inscribed\": item.get(\"date_inscribed\", \"\"),\n",
    "            \"secondary_dates\": item.get(\"secondary_dates\", \"\"),\n",
    "            \"revision\": item.get(\"revision\", \"\"),\n",
    "            \"extension\": item.get(\"extension\", \"\"),\n",
    "            \"unique_number\": item.get(\"unique_number\", \"\"),\n",
    "            \"http_url\": item.get(\"http_url\", \"\"),\n",
    "            \"image_url\": item.get(\"image_url\", \"\"),\n",
    "            \"wikidata_URI\": item.get(\"wikidata_URI\", \"\"), \n",
    "            \"wikidata_label\": item.get(\"wikidata_Label\", \"\"),\n",
    "            \"wikidata_instance_of_URI\": item.get(\"wikidata_instance_of_URI\", \"\"),\n",
    "            \"wikidata_instance_of_label\": item.get(\"wikidata_instance_of\", \"\"),\n",
    "            \"wikidata_culture_URI\": item.get(\"wikidata_culture_URI\", \"\"),\n",
    "            \"wikidata_culture_label\": item.get(\"wikidata_culture\", \"\"),\n",
    "            \"wikidata_area_in_sq_meters\": float(item.get(\"wikidata_area_in_sq_meters\", \"\"))\n",
    "        }\n",
    "    }\n",
    "   features.append(feature)\n",
    "\n",
    "# Create a FeatureCollection\n",
    "feature_collection = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": features\n",
    "}\n",
    "\n",
    "# Save GeoJSON to a file\n",
    "with open('area.geojson', 'w') as f:\n",
    "    geojson.dump(feature_collection, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
