{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Convert XML file from UNESCO site (https://whc.unesco.org/en/syndication) into dictionaries in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1, Step 1: loop through each element in each row and print all elements\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "tree = etree.parse('unesco-whs.xml')\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "for row in root:\n",
    "    for element in row:\n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1, Step 2: print element tags\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "tree = etree.parse('unesco-whs.xml')\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "for row in root:\n",
    "    for element in row:\n",
    "        print(element.tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1, Step 3: store elements and attributes in dictionaries and print\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "import json\n",
    "\n",
    "tree = etree.parse('unesco-whs.xml')\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "for row in root.findall('row'):\n",
    "    wh_site_info={}\n",
    "    wh_site_info['id_number'] = row.find('id_number').text\n",
    "    wh_site_info['site'] = row.find('site').text\n",
    "    wh_site_info['category'] = row.find('category').text\n",
    "    wh_site_info['criteria_txt'] = row.find('criteria_txt').text\n",
    "    wh_site_info['danger'] = row.find('danger').text\n",
    "    wh_site_info['date_inscribed'] = row.find('date_inscribed').text\n",
    "    wh_site_info['extension'] = row.find('extension').text\n",
    "    wh_site_info['http_url'] = row.find('http_url').text\n",
    "    wh_site_info['image_url'] = row.find('image_url').text\n",
    "    wh_site_info['iso_code'] = row.find('iso_code').text\n",
    "    wh_site_info['justification'] = row.find('justification').text\n",
    "    wh_site_info['latitude'] = row.find('latitude').text\n",
    "    wh_site_info['longitude'] = row.find('longitude').text\n",
    "    wh_site_info['location'] = row.find('location').text\n",
    "    wh_site_info['region'] = row.find('region').text\n",
    "    wh_site_info['revision'] = row.find('revision').text\n",
    "    wh_site_info['secondary_dates'] = row.find('secondary_dates').text\n",
    "    wh_site_info['short_description'] = row.find('short_description').text\n",
    "    wh_site_info['states'] = row.find('states').text\n",
    "    wh_site_info['transboundary'] = row.find('transboundary').text\n",
    "    wh_site_info['unique_number'] = row.find('unique_number').text\n",
    "    \n",
    "\n",
    "    print(wh_site_info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1, Step 4: dump dictionaries in a json file\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "import json\n",
    "\n",
    "tree = etree.parse('unesco-whs.xml')\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "# create list to store all wh_site dictionaries\n",
    "wh_sites = []\n",
    "\n",
    "for row in root.findall('row'):\n",
    "    wh_site_info={}\n",
    "    wh_site_info['id_number'] = row.find('id_number').text\n",
    "    wh_site_info['site'] = row.find('site').text\n",
    "    wh_site_info['category'] = row.find('category').text\n",
    "    wh_site_info['criteria_txt'] = row.find('criteria_txt').text\n",
    "    wh_site_info['danger'] = row.find('danger').text\n",
    "    wh_site_info['date_inscribed'] = row.find('date_inscribed').text\n",
    "    wh_site_info['extension'] = row.find('extension').text\n",
    "    wh_site_info['http_url'] = row.find('http_url').text\n",
    "    wh_site_info['image_url'] = row.find('image_url').text\n",
    "    wh_site_info['iso_code'] = row.find('iso_code').text\n",
    "    wh_site_info['justification'] = row.find('justification').text\n",
    "    wh_site_info['latitude'] = row.find('latitude').text\n",
    "    wh_site_info['longitude'] = row.find('longitude').text\n",
    "    wh_site_info['location'] = row.find('location').text\n",
    "    wh_site_info['region'] = row.find('region').text\n",
    "    wh_site_info['revision'] = row.find('revision').text\n",
    "    wh_site_info['secondary_dates'] = row.find('secondary_dates').text\n",
    "    wh_site_info['short_description'] = row.find('short_description').text\n",
    "    wh_site_info['states'] = row.find('states').text\n",
    "    wh_site_info['transboundary'] = row.find('transboundary').text\n",
    "    wh_site_info['unique_number'] = row.find('unique_number').text\n",
    "\n",
    "    wh_sites.append(wh_site_info)\n",
    "\n",
    "with open('unesco.json', 'w') as json_file:\n",
    "    json.dump(wh_sites, json_file, indent=2)\n",
    "\n",
    "# json.dump(all_sites,open('unesco.json', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: script that runs Wikidata SPARQL Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2, Step 1: print results from SPARQL query as JSON\n",
    "# query is copied over from https://query.wikidata.org/#%23CONCATINATED%20-%20INSTANCE%2C%20CULTURE%2C%20AREA%2C%20UNIT%2C%20AREA%20IN%20SQ%20METERS%20%28%0A%0ASELECT%20%3Fitem%20%3FitemLabel%20%3FWHID%20%28GROUP_CONCAT%28DISTINCT%20%3FinstanceOfLabel%3B%20SEPARATOR%3D%22%2C%20%22%29%20AS%20%3FWHS_instanceOf%29%20%0A%28GROUP_CONCAT%28DISTINCT%20%3FcultureLabel%3B%20SEPARATOR%3D%22%2C%20%22%29%20AS%20%3FWHS_culture%29%0A%28GROUP_CONCAT%28DISTINCT%20%3FareaInSqMeters%3B%20SEPARATOR%3D%22%2C%20%22%29%20AS%20%3FWHS_area_sq_meters%29%0AWHERE%0A%7B%0A%20%20%3Fitem%20wdt%3AP1435%20wd%3AQ9259.%20%23%20has%20heritage%20designation%20of%20World%20Heritage%20Site%0A%20%20%3Fitem%20wdt%3AP757%20%3FWHID.%0A%20%20optional%7B%0A%20%20%20%20%3Fitem%20wdt%3AP31%20%3FinstanceOf.%0A%20%20%20%20%7D%0A%20%20optional%7B%0A%20%20%3Fitem%20wdt%3AP2596%20%3Fculture%0A%20%20%20%20%7D%0A%20%20optional%7B%0A%20%20%3Fitem%20p%3AP2046%20%3Fstmnode.%20%23area%0A%20%20%3Fstmnode%20psv%3AP2046%20%3Fvaluenode.%0A%20%20%3Fvaluenode%20wikibase%3AquantityAmount%20%3Farea.%0A%20%20%3Fvaluenode%20wikibase%3AquantityUnit%20%3Funit.%0A%20%20%3Funit%20p%3AP2370%20%3Funitstmnode.%0A%20%20%3Funitstmnode%20psv%3AP2370%20%3Funitvaluenode.%20%0A%20%20%3Funitvaluenode%20wikibase%3AquantityAmount%20%3Fconversion.%0A%20%20%3Funitvaluenode%20wikibase%3AquantityUnit%20wd%3AQ25343.%0A%20%20BIND%28%3Farea%20%2a%20%3Fconversion%20AS%20%3FareaInSqMeters%29.%0A%20%20%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%0A%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22%5BAUTO_LANGUAGE%5D%2Cen%22.%20%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3FinstanceOf%20rdfs%3Alabel%20%3FinstanceOfLabel.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Fculture%20rdfs%3Alabel%20%3FcultureLabel.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%3Fitem%20rdfs%3Alabel%20%3FitemLabel.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7D%20%0A%7D%20%0A%0AGROUP%20BY%20%3Fitem%20%3FitemLabel%20%3FWHID%0AORDER%20BY%20%3FWHID%0A\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# SPARQL QUERY VARIABLE\n",
    "sparql = '''\n",
    "SELECT ?item ?itemLabel ?id_number \n",
    "(GROUP_CONCAT(DISTINCT ?instanceOfLabel; SEPARATOR=\", \") AS ?WHS_instanceOf) \n",
    "(GROUP_CONCAT(DISTINCT ?instanceOf; SEPARATOR=\", \") AS ?WHS_instanceOf_Q) \n",
    "(GROUP_CONCAT(DISTINCT ?cultureLabel; SEPARATOR=\", \") AS ?WHS_culture)\n",
    "(GROUP_CONCAT(DISTINCT ?culture; SEPARATOR=\", \") AS ?WHS_culture_Q)\n",
    "(GROUP_CONCAT(DISTINCT ?areaInSqMeters; SEPARATOR=\", \") AS ?WHS_area_sq_meters)\n",
    "WHERE\n",
    "{\n",
    "  ?item wdt:P1435 wd:Q9259. # has heritage designation of World Heritage Site\n",
    "  ?item wdt:P757 ?id_number.\n",
    "  optional{\n",
    "    ?item wdt:P31 ?instanceOf.\n",
    "    }\n",
    "  optional{\n",
    "  ?item wdt:P2596 ?culture\n",
    "    }\n",
    "   optional {\n",
    "        ?item p:P2046 ?statement.\n",
    "        ?statement psv:P2046 ?valuenode.\n",
    "        ?valuenode wikibase:quantityUnit ?unit.\n",
    "        ?valuenode wikibase:quantityAmount ?area.\n",
    "        ?statement pq:P518 wd:Q9259.\n",
    "        ?unit p:P2370 ?unitstmnode.\n",
    "        ?unitstmnode psv:P2370 ?unitvaluenode. \n",
    "        ?unitvaluenode wikibase:quantityAmount ?conversion.\n",
    "        ?unitvaluenode wikibase:quantityUnit wd:Q25343.\n",
    "        BIND(?area * ?conversion AS ?areaInSqMeters).\n",
    "  }\n",
    "                         \n",
    " SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". \n",
    "                        ?instanceOf rdfs:label ?instanceOfLabel.\n",
    "                        ?culture rdfs:label ?cultureLabel.\n",
    "                        ?item rdfs:label ?itemLabel.\n",
    "                        } \n",
    "} \n",
    "\n",
    "GROUP BY ?item ?itemLabel ?id_number\n",
    "ORDER BY ?id_number\n",
    "'''\n",
    "\n",
    "params = {\n",
    "    'query' : sparql\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Accept' : 'application/json',\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "r = requests.get(url, params = params, headers=headers)\n",
    "\n",
    "\n",
    "\n",
    "data = json.loads(r.text)\n",
    "\n",
    "results = []\n",
    "\n",
    "for result in data ['results']['bindings']:\n",
    "    results.append(result)\n",
    "\n",
    "results_json = json.dumps(results, indent =2)\n",
    "\n",
    "print(results_json)\n",
    "\n",
    "with open(\"sparql_query.json\", 'w') as json_file:\n",
    "    json.dump(results, json_file, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2, Step 2: print labels\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# SPARQL QUERY VARIABLE\n",
    "sparql = '''\n",
    "\n",
    "SELECT ?item ?itemLabel ?id_number \n",
    "(GROUP_CONCAT(DISTINCT ?instanceOfLabel; SEPARATOR=\", \") AS ?WHS_instanceOf) \n",
    "(GROUP_CONCAT(DISTINCT ?instanceOf; SEPARATOR=\", \") AS ?WHS_instanceOf_Q) \n",
    "(GROUP_CONCAT(DISTINCT ?cultureLabel; SEPARATOR=\", \") AS ?WHS_culture)\n",
    "(GROUP_CONCAT(DISTINCT ?culture; SEPARATOR=\", \") AS ?WHS_culture_Q)\n",
    "(GROUP_CONCAT(DISTINCT ?areaInSqMeters; SEPARATOR=\", \") AS ?WHS_area_sq_meters)\n",
    "WHERE\n",
    "{\n",
    "  ?item wdt:P1435 wd:Q9259. # has heritage designation of World Heritage Site\n",
    "  ?item wdt:P757 ?id_number.\n",
    "  optional{\n",
    "    ?item wdt:P31 ?instanceOf.\n",
    "    }\n",
    "  optional{\n",
    "  ?item wdt:P2596 ?culture\n",
    "    }\n",
    "   optional {\n",
    "        ?item p:P2046 ?statement.\n",
    "        ?statement psv:P2046 ?valuenode.\n",
    "        ?valuenode wikibase:quantityUnit ?unit.\n",
    "        ?valuenode wikibase:quantityAmount ?area.\n",
    "        ?statement pq:P518 wd:Q9259.\n",
    "        ?unit p:P2370 ?unitstmnode.\n",
    "        ?unitstmnode psv:P2370 ?unitvaluenode. \n",
    "        ?unitvaluenode wikibase:quantityAmount ?conversion.\n",
    "        ?unitvaluenode wikibase:quantityUnit wd:Q25343.\n",
    "        BIND(?area * ?conversion AS ?areaInSqMeters).\n",
    "  }\n",
    "                         \n",
    " SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". \n",
    "                        ?instanceOf rdfs:label ?instanceOfLabel.\n",
    "                        ?culture rdfs:label ?cultureLabel.\n",
    "                        ?item rdfs:label ?itemLabel.\n",
    "                        } \n",
    "} \n",
    "\n",
    "GROUP BY ?item ?itemLabel ?id_number\n",
    "ORDER BY ?id_number\n",
    "'''\n",
    "\n",
    "params = {\n",
    "    'query' : sparql\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Accept' : 'application/json',\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "r = requests.get(url, params = params, headers=headers)\n",
    "\n",
    "data = json.loads(r.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print data from SPARQL query JSON file\n",
    "# this was downloaded fromw wikidata\n",
    "\n",
    "import json\n",
    "\n",
    "file_path = \"wikidata_query_download.json\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sparql query json file with indents\n",
    "\n",
    "import json\n",
    "\n",
    "file_path = \"sparql_query.json\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "with open('wikidata.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import json \n",
    "merged = {}\n",
    "\n",
    "with open(\"unesco.json\") as f:\n",
    "    for line in f:\n",
    "        jsonified = json.loads(line)\n",
    "        merged[jsonified['id_number']] = jsonified \n",
    "\n",
    "with open(\"wikidata.json\") as f:\n",
    "    for line in f:\n",
    "        jsonified = json.loads(line)\n",
    "        merged[jsonified['id_number']].update(jsonified)\n",
    "\n",
    "merged = list(merged.values())\n",
    "print (merged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir(os.getcwd())\n",
    "file_path_1 = 'wikidata.json'\n",
    "file_path_2 = 'unesco.json'\n",
    "\n",
    "df1 = pd.read_json(file_path_1, lines=True)\n",
    "df2 = pd.read_json(file_path_2, lines=True)\n",
    "\n",
    "df = df1.merge(df2, on='id_number')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
